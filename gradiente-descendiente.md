### üìå **Gradiente Descendiente (Gradient Descent): Explicaci√≥n Te√≥rica y Matem√°tica**

---

### üöÄ **Concepto General**:

El **Gradiente Descendiente** es un m√©todo de optimizaci√≥n iterativo utilizado para minimizar una funci√≥n de costo o error en modelos de Machine Learning. El objetivo principal es encontrar los valores √≥ptimos para los par√°metros del modelo (pesos y sesgo) que minimicen la diferencia entre las predicciones del modelo y los valores reales.

---

### üìê **F√≥rmulas Matem√°ticas**:

1Ô∏è‚É£ **Funci√≥n de predicci√≥n**:

$$
y' = f(x) = wx + b
$$

-   $w$ ‚Üí Peso o pendiente.
-   $x$ ‚Üí Variable de entrada.
-   $b$ ‚Üí Sesgo o intersecci√≥n.

2Ô∏è‚É£ **Funci√≥n de costo (Cost Function)**:
En regresi√≥n lineal, la funci√≥n de costo m√°s com√∫n es el **Error Cuadr√°tico Medio (MSE)**:

$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^m (y_i' - y_i)^2
$$

-   $m$ ‚Üí N√∫mero de muestras.
-   $y_i'$ ‚Üí Predicci√≥n del modelo.
-   $y_i$ ‚Üí Valor real.

3Ô∏è‚É£ **Gradiente Descendiente**:
El gradiente descendiente ajusta los valores de $w$ y $b$ en cada iteraci√≥n para reducir el costo. Las actualizaciones se calculan as√≠:

Para el peso $w$:

$$
w := w - \alpha \frac{\partial J(w, b)}{\partial w}
$$

Para el sesgo $b$:

$$
b := b - \alpha \frac{\partial J(w, b)}{\partial b}
$$

Donde:

-   $\alpha$ ‚Üí Tasa de aprendizaje (learning rate).
-   $\frac{\partial J}{\partial w}$ y $\frac{\partial J}{\partial b}$ ‚Üí Derivadas parciales del costo con respecto a $w$ y $b$.

---

### üîç **C√°lculo de Derivadas**:

Para el peso $w$:

$$
\frac{\partial J}{\partial w} = \frac{1}{m} \sum_{i=1}^m (y_i' - y_i) x_i
$$

Para el sesgo $b$:

$$
\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (y_i' - y_i)
$$

---

### ‚úÖ **Ahora vamos a resolver el ejercicio proporcionado**:

**Datos del problema:**

-   Funci√≥n: $f(x) = wx + b$
-   Inicializaci√≥n: $w = 0$, $b = 0$, $\alpha = 0.01$

| **Sugar Levels (x)** | **Blood Glucose Levels (y)** |
| -------------------- | ---------------------------- |
| 3.75                 | 13.06                        |
| 9.51                 | 24.62                        |
| 7.32                 | 17.72                        |
| 5.99                 | 13.46                        |
| 1.56                 | -3.49                        |

---

### üîÑ **Voy a proceder a realizar el c√°lculo del Gradiente Descendiente paso a paso**:

1. Calcular√© las predicciones con $w = 0$ y $b = 0$.
2. Evaluar√© el error (MSE).
3. Calcular√© los gradientes ($\frac{\partial J}{\partial w}$ y $\frac{\partial J}{\partial b}$).
4. Actualizar√© los valores de $w$ y $b$.
5. Repetir√© el proceso por varias iteraciones.

---

He realizado el c√°lculo del Gradiente Descendiente durante 5 √©pocas SOLO PARA EL PRIMER PUNTO, y te he mostrado un historial completo de la evoluci√≥n de los par√°metros $w$, $b$, los gradientes ($dw$, $db$) y el error cuadr√°tico medio (MSE) en cada iteraci√≥n.

![alt text](image.png)

### Valores de `w` y `b` por √âpoca (Usando solo el primer punto)

| √âpoca | `w`      | `b`      |
| ----- | -------- | -------- |
| 0     | 0.489750 | 0.130600 |
| 1     | 0.905731 | 0.241528 |
| 2     | 1.259056 | 0.335748 |
| 3     | 1.559160 | 0.415776 |
| 4     | 1.814062 | 0.483750 |

### üîç **An√°lisis de los resultados:**

-   En las primeras iteraciones, los valores de $w$ y $b$ cambian de manera dr√°stica, lo cual es normal al inicio del entrenamiento.
-   La funci√≥n de costo (MSE) disminuye en cada iteraci√≥n, lo que indica que el modelo est√° aprendiendo.
-   El gradiente ($dw$ y $db$) va reduciendo su magnitud con el tiempo, lo cual es se√±al de que se est√° acercando a un m√≠nimo.

---
